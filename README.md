# Проект автоматизированной обработки данных с регулярными выражениями через Airflow и Docker

## Краткое описание проекта

Данный проект представляет собой комплект скриптов Python, использующий регулярные выражения (regex) для обработки данных, который интегрирован в Apache Airflow с использованием Docker для автоматизации и управления процессом ETL. Несмотря на то, что это не ETL в привычном понимании слова, реализация демонстрирует умение настраивать DAGs в Airflow и работать с Docker для решения задач автоматизации.

## Общая схема

- Взяты Python скрипты для обработки данных с применением regex из прошлого проекта.
- Создан Docker образ с необходимым окружением и добавлен на Docker Hub.
- Для автоматизации и планирования выполнения скриптов используется Apache Airflow.
- Использование Airflow DockerOperator для запуска скриптов в Docker контейнерах.

## Подготовка и использование

Для работы с данным проектом необходимо иметь установленные Docker и Apache Airflow. В данной заметке не будут рассматриваться детали установки этих компонентов, однако основные шаги по настройке проекта представлены ниже.

### Установка и настройка

1. Создание Docker образа:
   - Соберите Docker образ с необходимыми зависимостями из приложенного Dockerfile.
   - Загрузите собранный образ в ваш репозиторий на Docker Hub.

2. Настройка Airflow:
   - В Airflow необходимо установить переменную окружения AIRFLOW_REGEX_TASKS_DIR, указывающую на директорию проекта.
   - Поместите файл DAG в папку для DAGs, которая была создана при установке Airflow.
   - Перед запуском убедитесь, что в Airflow выставлены правильные настройки доступа к Docker, и Airflow может запускать контейнеры.

### Описание DAG

- DAG ID: airflow_regex_tasks
- Структура задач:
  - filter_ct_prefix --> extract_numeric_serials --> combine_serials_position --> match_serials
- Каждая задача выполняется в отдельном Docker контейнере с использованием образа, указанного в задаче.
- Данные между задачами передаются через Bind Mounts, примонтированный к контейнерам.
